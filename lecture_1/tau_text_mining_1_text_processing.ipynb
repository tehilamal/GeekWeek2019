{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tehilamal/GeekWeek2019/blob/master/lecture_1/tau_text_mining_1_text_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwK5-9FIB-lu"
      },
      "source": [
        "# Text Processing - for TAU Text Mining (for MBA) 24/25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1kiO9kACE6s"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7QG7sxmoCIvN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTfaCIzdCLPA"
      },
      "source": [
        "## 1. Importing the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMi51m_DNojx"
      },
      "source": [
        "Taken from:\n",
        "https://www.kaggle.com/datasets/maher3id/restaurant-reviewstsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NW0_Ooy_Nojx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCK6vQ5QCQJe"
      },
      "outputs": [],
      "source": [
        "dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter = '\\t', quoting = 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xUda9tONojx"
      },
      "outputs": [],
      "source": [
        "dataset.sample(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASUFBqgkNojx"
      },
      "source": [
        "### Q1: Extract random sentences\n",
        "\n",
        "Extract a random sample of 10 sentences as a list of strings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-DZTiPGNojx"
      },
      "source": [
        "## 2. Simple Bag-of-Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erh2d8EONojy"
      },
      "source": [
        "### Q2: Build a BoW-er\n",
        "\n",
        "Write a function that turns a string sentence into a dict-based BoW representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSP_h22KNojy"
      },
      "source": [
        "**Hint:** Use https://www.nltk.org/api/nltk.tokenize.word_tokenize.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4L9hgErNojy"
      },
      "outputs": [],
      "source": [
        "from typing import Dict\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def bower(sent_str: str) -> Dict[str, int]:\n",
        "    # Fix me!\n",
        "    return {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDk6pwA9Nojy"
      },
      "source": [
        "bower(sent_sample[5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pd9b1r-CNojy"
      },
      "source": [
        "### Q3: Generate a single BoW-dict representation for your sentence sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5Ecof3TNojy"
      },
      "source": [
        "### Q4: How many unique words in your sentence sample?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCQUaNfgNojy"
      },
      "source": [
        "## 3. BoW-based vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lk4VZJ9-Nojy"
      },
      "source": [
        "Here is how we can use existing `sklearn` features to get one-hot-encoded vector BoW representations of our sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OukzcY5YNojy"
      },
      "source": [
        "https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CaO-7WE2Nojy"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer(max_features = 3000, lowercase=False)\n",
        "X = cv.fit_transform(sent_sample).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59c--qPGNojy"
      },
      "outputs": [],
      "source": [
        "X[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a01wyLqNojz"
      },
      "source": [
        "### Q5: How many unique words in your sentence sample, according to CountVectorizer?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INw7pnWsNojz"
      },
      "source": [
        "Why is the number (probably) lower?\n",
        "\n",
        "Because of the way `CountVectorizer` separates strings into tokens; see the documentation for `token_pattern`:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK-duSH1Nojz"
      },
      "source": [
        "``token_patternstr or None, default=r”(?u)\\b\\w\\w+\\b”``\n",
        "\n",
        "  Regular expression denoting what constitutes a “token”, only used if analyzer == 'word'. The default regexp select tokens of 2 or\n",
        "  more alphanumeric characters **(punctuation is completely ignored and always treated as a token separator)**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qekztq71CixT"
      },
      "source": [
        "## 4. Text Preprocessing / Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wl2FCLtKNojz"
      },
      "source": [
        "### Q6: Build a document preprocessing function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94HKghWBNojz"
      },
      "source": [
        "Make sure to:\n",
        "1. Lowercae.\n",
        "2. Split into tokens.\n",
        "3. Remove stopwords.\n",
        "4. Stem the words into stems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqWhAywYNojz"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "def clean_doc(doc: str) -> List[str]:\n",
        "    # fix me!\n",
        "    return doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pv5eiYDVNojz"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGpP4CYcNoj0"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_l_wTlcHNoj0"
      },
      "outputs": [],
      "source": [
        "clean_samp = [clean_doc(doc) for doc in sent_sample]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3l_FclasNoj0"
      },
      "source": [
        "## 5. Text Preprocessing Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiBrViOsNoj0"
      },
      "source": [
        "### Q7: Build a coprpus preprocessing function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWUPtGSENoj0"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoNQxXnoNoj0"
      },
      "outputs": [],
      "source": [
        "def dataset_to_X(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    # Fix me!\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iSwbN7dNoj0"
      },
      "outputs": [],
      "source": [
        "dataset_to_X(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1tzp8MRNoj0"
      },
      "source": [
        "## 6. Text Preprocessing Pipeline w/ TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2b2UGXvNoj0"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vF_dDfiUNoj0"
      },
      "outputs": [],
      "source": [
        "def dataset_to_tfidf_X(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    # Fix me!\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K75_LfvzNoj4"
      },
      "outputs": [],
      "source": [
        "tfidf_X = dataset_to_tfidf_X(dataset)\n",
        "tfidf_X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXsMknntNoj4"
      },
      "outputs": [],
      "source": [
        "def present_active_indices(X: pd.DataFrame, index: int) -> pd.Series:\n",
        "    \"\"\"Returns a sub-series of non-zero components of the document vector at the given index.\"\"\"\n",
        "    return X.iloc[index][X.iloc[index] != 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1K6CefzNoj4"
      },
      "outputs": [],
      "source": [
        "present_active_indices(tfidf_X, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "o7nY0pniNoj4"
      },
      "outputs": [],
      "source": [
        "present_active_indices(tfidf_X, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1twlJJtzNoj5"
      },
      "source": [
        "## 7. Use our pipeline for some simple text classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CS2T_ztNoj5"
      },
      "source": [
        "We will use the `\"Liked\"` column (which is either 0 or 1) as our label, and see if we can learn to predict it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLqmAkANCp1-"
      },
      "source": [
        "### Setup our X and y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qroF7XcSCvY3"
      },
      "outputs": [],
      "source": [
        "y = dataset.iloc[:, -1].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyB8cNzrNoj5"
      },
      "outputs": [],
      "source": [
        "y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH_VjgPzC2cd"
      },
      "source": [
        "### Split the dataset into a training and test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQXYM5VzDDDI"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(tfidf_X, y, test_size = 0.20, random_state = 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkIq23vEDIPt"
      },
      "source": [
        "### Fit a Gaussian Naive Bayes classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DS9oiDXXDRdI"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "classifier = GaussianNB()\n",
        "classifier.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JaRM7zXDWUy"
      },
      "source": [
        "### Predict on the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMvrDREXNoj6"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iif0CVhFDaMp",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "y_pred = classifier.predict(X_test)\n",
        "# print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoMltea5Dir1"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhsLmF7cNoj6"
      },
      "outputs": [],
      "source": [
        "# taken from https://gist.github.com/shaypal5/94c53d765083101efc0240d776a23823\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n",
        "    \"\"\"Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.\n",
        "\n",
        "    Note that due to returning the created figure object, when this funciton is called in a\n",
        "    notebook the figure willl be printed twice. To prevent this, either append ; to your\n",
        "    function call, or modify the function by commenting out the return expression.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    confusion_matrix: numpy.ndarray\n",
        "        The numpy.ndarray object returned from a call to sklearn.metrics.confusion_matrix.\n",
        "        Similarly constructed ndarrays can also be used.\n",
        "    class_names: list\n",
        "        An ordered list of class names, in the order they index the given confusion matrix.\n",
        "    figsize: tuple\n",
        "        A 2-long tuple, the first value determining the horizontal size of the ouputted figure,\n",
        "        the second determining the vertical size. Defaults to (10,7).\n",
        "    fontsize: int\n",
        "        Font size for axes labels. Defaults to 14.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    matplotlib.figure.Figure\n",
        "        The resulting confusion matrix figure\n",
        "    \"\"\"\n",
        "    df_cm = pd.DataFrame(\n",
        "        confusion_matrix, index=class_names, columns=class_names,\n",
        "    )\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "    try:\n",
        "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
        "    except ValueError:\n",
        "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
        "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
        "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    # Note that due to returning the created figure object, when this funciton is called in a notebook\n",
        "    # the figure willl be printed twice. To prevent this, either append ; to your function call, or\n",
        "    # modify the function by commenting out this return expression.\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xj9IU6MxDnvo"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "cm = confusion_matrix(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZjchnDbNoj6"
      },
      "outputs": [],
      "source": [
        "print_confusion_matrix(cm, ['Not Liked', 'Liked']);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWvYtAiwNoj6"
      },
      "outputs": [],
      "source": [
        "acc = accuracy_score(y_test, y_pred)\n",
        "acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZbqBh8GNoj7"
      },
      "outputs": [],
      "source": [
        "print(f\"We achieve an accuracy score of {100*acc:.2f}%.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZiip6DwNoj7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}